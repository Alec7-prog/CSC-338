{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85b65288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+math.exp(-x))\n",
    "\n",
    "def sigmoidderivative(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))\n",
    "\n",
    "vec_sigmoid = np.vectorize(sigmoid)\n",
    "vec_sigmoidderivative = np.vectorize(sigmoidderivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4d98633",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLayer:\n",
    "\n",
    "    def __init__(self, numinputs:int, numoutputs:int, activation=None):\n",
    "\n",
    "        self.numinputs = numinputs\n",
    "        self.numoutputs = numoutputs\n",
    "        self.activation = activation\n",
    "        self.weights = np.random.randn(self.numoutputs, self.numinputs + 1)\n",
    "\n",
    "    \n",
    "    def Evaluate(self, inputs):\n",
    "\n",
    "        inputs = np.append(inputs, np.array([1]))\n",
    "    \n",
    "        outputs = self.weights @ inputs # this is \\vec{h}\n",
    "\n",
    "        match self.activation:\n",
    "            case \"Sigmoid\":\n",
    "                outputs = vec_sigmoid(outputs)\n",
    "\n",
    "            case \"Softmax\":\n",
    "                denom = 0\n",
    "                for i in range(len(outputs)):\n",
    "                    denom += math.exp[outputs[i]]\n",
    "                    outputs[i] = math.exp(outputs[i])\n",
    "                outputs = outputs/denom\n",
    "\n",
    "            case \"ReLU\":\n",
    "                # Put code here, delete the pass\n",
    "                pass #Alejandro shall not pass\n",
    "            \n",
    "            case \"Tanh\":\n",
    "                # Put code here, delete the pass\n",
    "                pass\n",
    "\n",
    "\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    \n",
    "        \n",
    "    def ComputeLocalGradient(self, inputs):\n",
    "        # z is output after activation\n",
    "        # h is output after linear layer\n",
    "        # w are weights\n",
    "        # Need to compute three things:\n",
    "        # dz/dh\n",
    "        # dh/dw\n",
    "        # dh/dx\n",
    "\n",
    "        inputs = np.append(inputs, np.array([1]))\n",
    "        outputs = self.weights @ inputs\n",
    "\n",
    "\n",
    "        # This part computes dzdh, and has cases for various activation functions\n",
    "        match self.activation:\n",
    "            case \"Sigmoid\":\n",
    "                dzdh = np.diag(vec_sigmoidderivative(outputs))\n",
    "            case \"Softmax\":\n",
    "                n = len(outputs)\n",
    "                dzdh = np.zeros((n, n))\n",
    "                denom = 0\n",
    "                for i in range(n):\n",
    "                    denom += math.exp(outputs[i])\n",
    "                \n",
    "                for i in range(n):\n",
    "                    for j in range(n):\n",
    "                        if i == j:\n",
    "                            dzdh[i][j] = (denom * math.exp(outputs[i]) - (math.exp(outputs[i])**2))/(denom**2)\n",
    "                        else:\n",
    "                            dzdh[i][j] = -(math.exp(outputs[j]))*(math.exp(outputs[j]))/(denom**2)\n",
    "\n",
    "            case \"ReLU\":\n",
    "                # Put code here and remove pass\n",
    "                pass\n",
    "            case \"Tanh\":\n",
    "                # Put code here and remove pass\n",
    "                pass\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # This part computes dhdw        \n",
    "        dhdw = np.zeros((self.numoutputs, self.numoutputs, self.numinputs+1)) #because of bias\n",
    "        for i in range(self.numoutputs):\n",
    "            for j in range(self.numinputs):\n",
    "                dhdw[i,i,j] = inputs[i]\n",
    "            dhdw[i,i,self.numinputs] = 1\n",
    "            \n",
    "        # This part computes dhdx\n",
    "        dhdx = self.weights[:, :-1]\n",
    "\n",
    "\n",
    "        return (dzdh, dhdw, dhdx)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35480923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "(3, 3, 6)\n",
      "(3, 5)\n"
     ]
    }
   ],
   "source": [
    "Layer1 = NeuralLayer(5,3,\"Sigmoid\")\n",
    "#Layer1.weights = np.array([[1, 2, -1], [3, -2, 1]])\n",
    "test1 = np.array([1, 2,3,4,5])\n",
    "(dzdh, dhdw, dhdx) = Layer1.ComputeLocalGradient(test1)\n",
    "print(dzdh.shape)\n",
    "print(dhdw.shape)\n",
    "print(dhdx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79a744c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, errorfunc=None):\n",
    "        \n",
    "        self.errorfunc = errorfunc\n",
    "        self.layers = []\n",
    "        self.numlayers = 0\n",
    "\n",
    "    def AppendLayer(self, layer: NeuralLayer):\n",
    "        # need to check that the new layer to be appended has same \n",
    "        # number of inputs as the last layer already in the network\n",
    "        if len(self.layers) > 0:\n",
    "            if layer.numinputs == self.layers[-1].numoutputs:\n",
    "                self.layers.append(layer)\n",
    "                self.numlayers += 1\n",
    "            else:\n",
    "                print(\"Error: number of inputs does not match previous layer\")\n",
    "        else:\n",
    "            self.layers.append(layer)\n",
    "            self.numlayers += 1\n",
    "\n",
    "        \n",
    "    def Evaluate(self, inputs):\n",
    "\n",
    "        outputs = []\n",
    "        outputs.append(self.layers[0].Evaluate(inputs))\n",
    "\n",
    "        for i in range(1,self.numlayers):\n",
    "            outputs.append(self.layers[i].Evaluate(outputs[i-1]))\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    def ComputeError(self, inputs, trueoutputs):\n",
    "\n",
    "        outputs = self.Evaluate(inputs)\n",
    "        \n",
    "        if self.errorfunc == \"MSE\":\n",
    "            n = len(outputs[-1])\n",
    "            diffs = outputs[-1] - trueoutputs\n",
    "            err = np.dot(diffs, diffs)\n",
    "            err = err/(2*n)\n",
    "            return err\n",
    "\n",
    "    def BackPropagate(self, inputs, trueoutputs, learningrate):\n",
    "\n",
    "        outputs = self.Evaluate(inputs)\n",
    "        gradients = []\n",
    "\n",
    "        # Compute all the necessary gradients\n",
    "        for i in range(self.numlayers):\n",
    "            if i == 0:\n",
    "                tempinput = inputs\n",
    "            else:\n",
    "                tempinput = outputs[i-1]\n",
    "            \n",
    "            gradients.append(self.layers[i].ComputeLocalGradient(tempinput))\n",
    "\n",
    "        match self.errorfunc:\n",
    "            case \"MSE\":\n",
    "                dldz = (0.5) * (outputs[-1] - trueoutputs)\n",
    "\n",
    "            case \"CrossEntropy\":\n",
    "                dldz = np.zeros(len(trueoutputs))\n",
    "                spot = np.where(1 == trueoutputs)\n",
    "                dldz[spot] = 1/outputs[-1][spot]\n",
    "                \n",
    "            \n",
    "\n",
    "        # Update weights, working backwards\n",
    "\n",
    "        currgrad = dldz @ gradients[-1][0]\n",
    "    \n",
    "        for i in range(self.numlayers-1, -1, -1):\n",
    "            self.layers[i].weights -= learningrate * (currgrad @ gradients[i][1])\n",
    "            currgrad = currgrad @ gradients[i][0] @ gradients[i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eef2ccb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05475984808485329\n",
      "0.044425528421241436\n",
      "0.036856884468483966\n",
      "0.031213273133336498\n",
      "0.026909518385108064\n",
      "0.023552582652207502\n",
      "0.020878883960718863\n",
      "0.018709246454045758\n",
      "0.016919473893688846\n",
      "0.015421641025180333\n",
      "0.014152193458360059\n"
     ]
    }
   ],
   "source": [
    "MyNN = NeuralNetwork(errorfunc=\"MSE\")\n",
    "MyLayer1 = NeuralLayer(5, 3, \"Sigmoid\")\n",
    "MyLayer2 = NeuralLayer(3, 2, \"Sigmoid\")\n",
    "\n",
    "\n",
    "MyNN.AppendLayer(MyLayer1)\n",
    "MyNN.AppendLayer(MyLayer2)\n",
    "\n",
    "myinput = np.array([1,2,3,4,5])\n",
    "mytrue = np.array([1,0])\n",
    "\n",
    "print(MyNN.ComputeError(myinput, mytrue))\n",
    "\n",
    "for i in range(10):\n",
    "    MyNN.BackPropagate(myinput, mytrue, 1)\n",
    "    print(MyNN.ComputeError(myinput, mytrue))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6b2becb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from MNIST database\n",
    "(x_train0, y_train0), (x_test0, y_test0) = tf.keras.datasets.mnist.load_data()\n",
    "assert x_train0.shape == (60000, 28, 28)\n",
    "assert x_test0.shape == (10000, 28, 28)\n",
    "assert y_train0.shape == (60000,)\n",
    "assert y_test0.shape == (10000,)\n",
    "\n",
    "# Prepare data for processing\n",
    "# x_train and x_test need to be reshaped and converted to np.float64\n",
    "# y_train and y_test need to be one-hot encoded\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d12a2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
       "       126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "       253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
       "       253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
       "       253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "       241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
       "       253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
       "       183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "       229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
       "       221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
       "       213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
       "       219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
       "       226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train0[0].reshape(28*28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0adc12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29992111279756706\n",
      "0.29872659842536653\n",
      "0.29789246407648473\n",
      "0.29718726984122323\n",
      "0.29650110057785967\n",
      "0.2957601628771834\n",
      "0.29489302319434085\n",
      "0.29380725948182473\n",
      "0.29235948562576364\n",
      "0.2903015382042875\n",
      "0.2871753356703641\n",
      "0.2821393154787718\n",
      "0.27399181634739167\n",
      "0.26298898185150366\n",
      "0.25399179804439903\n",
      "0.2494410281597695\n",
      "0.2466717321601936\n",
      "0.24403649060472113\n",
      "0.24062631752107572\n",
      "0.23541662013471681\n",
      "0.22716710446611366\n",
      "0.21645659371669881\n",
      "0.20842926097092027\n",
      "0.20496594901250315\n",
      "0.20347895827192763\n",
      "0.2026964137358366\n",
      "0.20221721730805112\n",
      "0.2018928998909902\n",
      "0.20165785564034447\n",
      "0.20147895600379867\n",
      "0.20133772831427588\n",
      "0.20122306410623567\n",
      "0.2011278769358237\n",
      "0.20104742725530586\n",
      "0.20097842038555166\n",
      "0.2009184914266954\n",
      "0.2008658964021824\n",
      "0.20081931926796268\n",
      "0.20077774699451786\n",
      "0.2007403862102244\n",
      "0.20070660608083984\n",
      "0.20067589824224252\n",
      "0.20064784810750366\n",
      "0.2006221139366558\n",
      "0.2005984113142171\n",
      "0.20057650146418685\n",
      "0.20055618233407393\n",
      "0.2005372817075472\n",
      "0.20051965182399584\n",
      "0.20050316513173488\n",
      "0.2004877109040466\n",
      "0.20047319251903\n",
      "0.20045952525523963\n",
      "0.20044663449180872\n",
      "0.2004344542285006\n",
      "0.20042292586083715\n",
      "0.2004119971601213\n",
      "0.20040162141919735\n",
      "0.2003917567331583\n",
      "0.20038236539060644\n",
      "0.20037341335601347\n",
      "0.20036486982755766\n",
      "0.20035670685782242\n",
      "0.20034889902710384\n",
      "0.20034142316095221\n",
      "0.20033425808507022\n",
      "0.2003273844118918\n",
      "0.20032078435413414\n",
      "0.20031444156140368\n",
      "0.20030834097657685\n",
      "0.20030246870920104\n",
      "0.20029681192359403\n",
      "0.2002913587396767\n",
      "0.20028609814487006\n",
      "0.20028101991563335\n",
      "0.20027611454742816\n",
      "0.20027137319206437\n",
      "0.2002667876015319\n",
      "0.20026235007754187\n",
      "0.20025805342610936\n",
      "0.20025389091659548\n",
      "0.20024985624470334\n",
      "0.200245943498988\n",
      "0.20024214713049368\n",
      "0.20023846192518255\n",
      "0.20023488297885733\n",
      "0.2002314056743178\n",
      "0.20022802566052053\n",
      "0.20022473883354044\n",
      "0.20022154131915143\n",
      "0.2002184294568698\n",
      "0.20021539978531605\n",
      "0.2002124490287708\n",
      "0.20020957408481105\n",
      "0.2002067720129269\n",
      "0.20020404002402867\n",
      "0.2002013754707639\n",
      "0.2001987758385717\n",
      "0.2001962387374096\n",
      "0.2001937618940946\n",
      "0.20019134314520498\n",
      "0.20019134314520498\n",
      "Final check evaluation: [array([1.96103695e-02, 9.99928755e-01, 3.30314730e-02, 9.99998469e-01,\n",
      "       3.60460901e-02, 9.62089140e-01, 9.99988524e-01, 2.94568844e-10,\n",
      "       9.99891575e-01, 2.31110357e-07])]\n"
     ]
    }
   ],
   "source": [
    "MyMNISTNetwork = NeuralNetwork(\"MSE\")\n",
    "MyMNISTNetwork.AppendLayer(NeuralLayer(28*28,10,\"Sigmoid\"))\n",
    "\n",
    "\n",
    "y_train0[0]\n",
    "\n",
    "#testinput = np.astype(x_train0[0].reshape(28*28), np.float64)\n",
    "testinput = x_train0[0].reshape(28*28)\n",
    "testinput = testinput.astype(np.float64)\n",
    "\n",
    "testinput /= 255.0\n",
    "#print(testinput.sum())\n",
    "#print(MyMNISTNetwork.Evaluate(testinput))\n",
    "#print(MyMNISTNetwork.layers[-1].weights.dtype)\n",
    "\n",
    "onehot = np.array([0,0,0,0,0,1,0,0,0,0])\n",
    "\n",
    "print(MyMNISTNetwork.ComputeError(testinput, onehot))\n",
    "for i in range(100):\n",
    "    MyMNISTNetwork.BackPropagate(testinput, onehot, 10)\n",
    "    print(MyMNISTNetwork.ComputeError(testinput, onehot))\n",
    "    \n",
    "print(MyMNISTNetwork.ComputeError(testinput, onehot))\n",
    "\n",
    "print(\"Final check evaluation: \" + str(MyMNISTNetwork.Evaluate(testinput)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cae961e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
